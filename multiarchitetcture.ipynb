{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 04:43:29.281215: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-24 04:43:35.891332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper_functions import lr_schedule\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_DIMS = (256, 256, 3)\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    drop = 0.5\n",
    "    epoch_drop = 2.0\n",
    "    lr = initial_lr * (drop ** (epoch // epoch_drop))\n",
    "    print(f\"Epoch {epoch}: Learning rate = {lr}\")\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 83489 files belonging to 4 classes.\n",
      "Using 66792 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 04:43:34.183646: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.206757: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.206809: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.211808: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.211993: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.212014: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.000489: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.000525: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.000531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-12-24 04:43:34.000552: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-24 04:43:34.000574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 files belonging to 4 classes.\n",
      "Using 6 files for validation.\n",
      "Class names: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n"
     ]
    }
   ],
   "source": [
    "train_dir = r'/home/arsenal/ml/OCT2017/train'\n",
    "val_dir = r'/home/arsenal/ml/OCT2017/val'\n",
    "\n",
    "train_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(256,256),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(256,256),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">31,509,697</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">31,509,955</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ sequential_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │ \u001b[38;5;34m31,509,697\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │ \u001b[38;5;34m31,509,955\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ sequential_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,019,652</span> (240.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m63,019,652\u001b[0m (240.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,019,652</span> (240.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m63,019,652\u001b[0m (240.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorboard\n",
    "\n",
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=IMAGE_DIMS),\n",
    "    keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model1.compile(optimizer=Adam(),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=IMAGE_DIMS),\n",
    "    keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=Adam(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "input = keras.layers.Input(shape=IMAGE_DIMS)\n",
    "\n",
    "model1_output = model1(input)\n",
    "model2_output = model2(input)\n",
    "\n",
    "combined_output = keras.layers.Concatenate()([model1_output, model2_output])\n",
    "final_model = keras.Model(inputs=input, outputs=combined_output)\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard('assets/logs/multiModel')\n",
    "\n",
    "final_model.compile(optimizer=Adam(),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate = 0.001\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735015416.517529    7195 service.cc:145] XLA service 0x7f76c400fdf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1735015416.517610    7195 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2024-12-24 04:43:36.579126: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-24 04:43:36.858850: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "2024-12-24 04:43:45.838004: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng56{k2=0,k13=2,k14=3} for conv (f32[32,32,127,127]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,125,125]{3,2,1,0}, f32[64,32,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "2024-12-24 04:43:45.860193: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 6.590583263s\n",
      "Trying algorithm eng56{k2=0,k13=2,k14=3} for conv (f32[32,32,127,127]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,125,125]{3,2,1,0}, f32[64,32,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "I0000 00:00:1735015421.408573    7195 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2041/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - accuracy: 0.4448 - loss: 4.3550"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 04:47:01.048765: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[8,64,125,125]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,127,127]{3,2,1,0}, f32[64,32,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "2024-12-24 04:47:01.081878: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 6.589869999s\n",
      "Trying algorithm eng0{} for conv (f32[8,64,125,125]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,127,127]{3,2,1,0}, f32[64,32,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 93ms/step - accuracy: 0.4448 - loss: 4.3550 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 0.0010\n",
      "Epoch 1: Learning rate = 0.001\n",
      "Epoch 2/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 92ms/step - accuracy: 0.4459 - loss: 4.3248 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 0.0010\n",
      "Epoch 2: Learning rate = 0.0005\n",
      "Epoch 3/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 90ms/step - accuracy: 0.4461 - loss: 4.3206 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 5.0000e-04\n",
      "Epoch 3: Learning rate = 0.0005\n",
      "Epoch 4/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 91ms/step - accuracy: 0.4462 - loss: 4.3237 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 5.0000e-04\n",
      "Epoch 4: Learning rate = 0.00025\n",
      "Epoch 5/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 92ms/step - accuracy: 0.4464 - loss: 4.3165 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 2.5000e-04\n",
      "Epoch 5: Learning rate = 0.00025\n",
      "Epoch 6/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 92ms/step - accuracy: 0.4461 - loss: 4.3248 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 2.5000e-04\n",
      "Epoch 6: Learning rate = 0.000125\n",
      "Epoch 7/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 96ms/step - accuracy: 0.4462 - loss: 4.3225 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 1.2500e-04\n",
      "Epoch 7: Learning rate = 0.000125\n",
      "Epoch 8/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 88ms/step - accuracy: 0.4461 - loss: 4.3219 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 1.2500e-04\n",
      "Epoch 8: Learning rate = 6.25e-05\n",
      "Epoch 9/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 91ms/step - accuracy: 0.4464 - loss: 4.3212 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 6.2500e-05\n",
      "Epoch 9: Learning rate = 6.25e-05\n",
      "Epoch 10/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 93ms/step - accuracy: 0.4462 - loss: 4.3225 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 6.2500e-05\n",
      "Epoch 10: Learning rate = 3.125e-05\n",
      "Epoch 11/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 93ms/step - accuracy: 0.4461 - loss: 4.3210 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 3.1250e-05\n",
      "Epoch 11: Learning rate = 3.125e-05\n",
      "Epoch 12/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 93ms/step - accuracy: 0.4460 - loss: 4.3262 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 3.1250e-05\n",
      "Epoch 12: Learning rate = 1.5625e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 93ms/step - accuracy: 0.4463 - loss: 4.3201 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 1.5625e-05\n",
      "Epoch 13: Learning rate = 1.5625e-05\n",
      "Epoch 14/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 93ms/step - accuracy: 0.4462 - loss: 4.3241 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 1.5625e-05\n",
      "Epoch 14: Learning rate = 7.8125e-06\n",
      "Epoch 15/15\n",
      "\u001b[1m2088/2088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 93ms/step - accuracy: 0.4464 - loss: 4.3194 - val_accuracy: 0.3333 - val_loss: 3.2640 - learning_rate: 7.8125e-06\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(dataset, num_classes):\n",
    "    def encode(image, label):\n",
    "        label = tf.one_hot(label, num_classes)\n",
    "        return image, label\n",
    "    return dataset.map(encode)\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "train_dataset_encoded = one_hot_encode(train_dataset, num_classes)\n",
    "val_dataset_encoded = one_hot_encode(val_dataset, num_classes)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "if not final_model.built:\n",
    "    final_model.compile(optimizer=Adam(),\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "final_model.build(input_shape=(None, *IMAGE_DIMS))\n",
    "\n",
    "history = final_model.fit(train_dataset_encoded,\n",
    "                          epochs=15,\n",
    "                          validation_data=val_dataset_encoded,\n",
    "                          batch_size = 32,\n",
    "                          callbacks=[lr_scheduler,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (4.25.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from tensorboard) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/arsenal/miniconda3/envs/tf/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aa8a5b698bbbbb10\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aa8a5b698bbbbb10\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=assets/logs/multiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load the image you want to predict\n",
    "img_path = '/home/arsenal/ml/DRUSEN-9837663-1.jpeg'  # Update with your image path\n",
    "\n",
    "# Load the image and resize it to the target size used during training (256, 256)\n",
    "img = image.load_img(img_path, target_size=(256, 256), color_mode='grayscale')\n",
    "\n",
    "# Step 3: Preprocess the image\n",
    "# Convert the image to a numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Normalize the image (if your training data was normalized by dividing by 255)\n",
    "img_array = img_array / 255.0  # Scale pixel values to [0, 1]\n",
    "\n",
    "# Reshape the image to match the input shape the model expects (add batch dimension)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Shape will be (1, 256, 256, 1)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Step 5: Process the predictions\n",
    "# For multi-class classification, predictions will be a probability distribution over all classes\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]  # Get the index of the highest probability class\n",
    "\n",
    "# Step 6: Map the predicted class to class names\n",
    "class_names = ['Class1', 'Class2', 'Class3', 'Class4']  # Replace with your actual class names\n",
    "predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "# Step 7: Display the result\n",
    "print(f'Predicted class index: {predicted_class}')\n",
    "print(f'Predicted class name: {predicted_class_name}')\n",
    "\n",
    "# Optional: Display the image along with the prediction\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f'Predicted Class: {predicted_class_name}')\n",
    "plt.axis('off')  # Hide axes for clarity\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
